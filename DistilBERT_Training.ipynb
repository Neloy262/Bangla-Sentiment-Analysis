{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcP2rAAkVUc0"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh2S-QAIq3CP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizerFast\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYzan7muWUrM"
      },
      "outputs": [],
      "source": [
        "p_df = pd.read_csv(\"positive.csv\")\n",
        "n_df = pd.read_csv(\"negative.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGIITbDrrM9E"
      },
      "outputs": [],
      "source": [
        "p_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYWrR1KJrWUK"
      },
      "outputs": [],
      "source": [
        "n_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vETrnqedrYUH"
      },
      "outputs": [],
      "source": [
        "def get_training_data(p_df,n_df):\n",
        "  \n",
        "  train_texts = [*p_df['text'] , *n_df['text']]\n",
        "  train_labels = [*p_df['label'] , *n_df['label']]\n",
        "  \n",
        "  return train_texts, train_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4esgroAjt8S0"
      },
      "outputs": [],
      "source": [
        "train_texts,train_labels = get_training_data(p_df,n_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISWHP6ndtN9y"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwsmi1Yzz0xV"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyQsIfWM0KYv"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8HxWjNk0N_I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class BanglaDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = BanglaDataset(train_encodings, train_labels)\n",
        "val_dataset = BanglaDataset(val_encodings, val_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJhvWb7b0VpZ"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/test\")"
      ],
      "metadata": {
        "id": "9Ticqa3YhAKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = DistilBertForSequenceClassification.from_pretrained(\"/content/results/checkpoint-500\")\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IZQZMr-YaJo",
        "outputId": "4bedf801-21b3-452a-f527-2613f82ae8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/results/checkpoint-500/config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/results/checkpoint-500/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at /content/results/checkpoint-500.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = pipeline('sentiment-analysis', model=test_model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "WJPhwKQfh_Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = nlp([\"‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ú‡ßÄ‡¶¨‡¶®‡ßá‡¶∞ ‡¶∏‡¶¨‡¶•‡ßá‡¶ï‡ßá ‡¶´‡¶æ‡¶≤‡¶§‡ßÅ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶®‡¶æ‡¶ü‡¶ï ‡¶è‡¶á‡¶ü‡¶æ\"])"
      ],
      "metadata": {
        "id": "VE6BPwYGiLJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for result in results:\n",
        "  print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxGR9X4HibVH",
        "outputId": "8e5f6309-29f9-47c6-d517-8e251b125ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'LABEL_0', 'score': 0.9052820801734924}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "ZHX2MC4G2lDF",
        "outputId": "50b2cdac-14bc-451d-8f77-449d803119ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 2362\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37/37 00:36]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 3.0,\n",
              " 'eval_loss': 0.23698624968528748,\n",
              " 'eval_runtime': 37.5094,\n",
              " 'eval_samples_per_second': 62.971,\n",
              " 'eval_steps_per_second': 0.986}"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ESxDV9tK5gds"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DistilBERT_Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}